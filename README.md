# MindScreen - A virtual Air Screen 
##Brief Overview 
Developed a touchless interaction system that replicates screen control through webcam-based input. Designed with accessibility in mind, empowering individuals, particularly those with Parkinsonâ€™s,  to navigate digital interfaces without the need for physical devices.

## ðŸ§° Tech Stack 

Python â€“ A Core scripting language for system logic and automation
MediaPipe â€“ Used for real-time skeletal tracking and gesture recognition
OpenAI APIs â€“ Power intelligent decision-making and adaptive behavior
PyAutoGUI â€“ Simulates user input to replicate touchless interaction

Chosen for their balance between functionality and performance, these tools ensure smooth operation even on low-end systems 


## ðŸ§©System Architecture 
The system operates through a streamlined, three-stage pipeline:

1. Input Capture â†’ 2. AI Interpretation â†’ 3. Virtual Interaction

Input Capture: Utilizes a webcam to track hand movements and gestures continuously.

AI Interpretation: Processes visual data to interpret user intent through gesture recognition and contextual AI models.

Virtual Interaction: Translates interpreted gestures into simulated mouse and keyboard actions for real-time control of digital interfaces.

Each component is designed for low-latency performance, utilizing lightweight, open-source technologies to ensure reliability even on resource-constrained devices.


## â™¿ Accessibility Impact
Parkinsonâ€™s disease affects over 10 million people globally, many of whom face challenges with fine motor control. MindScreen offers a cost-effective alternative to traditional assistive technologies by enabling users to interact with computers without physical input devices. By reducing reliance on caregivers and eliminating hardware barriers, it empowers greater digital independence and inclusion for individuals with motor impairments.

